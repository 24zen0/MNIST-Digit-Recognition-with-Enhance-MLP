{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6093080",
   "metadata": {},
   "source": [
    "The highest should be around 99.40-99.50%, and takes about 55 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c471786",
   "metadata": {},
   "source": [
    "Set the random seed:\n",
    "although a seed is setted, due to problems like cpu running, batchnorm,OneCycleLR and so on, there still remains slight fluxuation among different trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2314b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the highest could reach 99.34%!!Epoch 114: Val Acc: 99.34%\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import os\n",
    "\n",
    "# ==================== 确定性设置 ====================\n",
    "SEED = 42\n",
    "\n",
    "def set_deterministic(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "set_deterministic(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4987d",
   "metadata": {},
   "source": [
    "getting data, and also reinforcing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d3d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders():\n",
    "    # 增强训练集变换\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomAffine(\n",
    "            degrees=3,\n",
    "            translate=(0.05, 0.05),\n",
    "            shear=5,\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "        transforms.Lambda(lambda x: x.view(-1))\n",
    "    ])\n",
    "    \n",
    "    # 测试集使用基础变换\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "        transforms.Lambda(lambda x: x.view(-1))\n",
    "    ])\n",
    "\n",
    "    # 固定数据集划分\n",
    "    full_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "    train_indices = list(range(55000))\n",
    "    val_indices = list(range(55000, 60000))\n",
    "    \n",
    "    train_dataset = Subset(full_train, train_indices)\n",
    "    val_dataset = Subset(\n",
    "        datasets.MNIST(root='./data', train=True, download=True, transform=transform_test),\n",
    "        val_indices\n",
    "    )\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    # 确定性DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=64, shuffle=False,\n",
    "        num_workers=0, worker_init_fn=lambda _: np.random.seed(SEED)\n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=0)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56331e4a",
   "metadata": {},
   "source": [
    "I used swish as an activateion function, it is smoother than relu and has a better stablilty than others. I changed the cross entropy loss into the focal loss,which is better at finding the complex samples. I increased the learning depth to 2048, so I also had to add the batchnorm to  help it converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee07a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 模型定义 ====================\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Parameter(torch.tensor(float(beta)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(self.beta * x)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=1.5, alpha=0.25):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        return (self.alpha * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "\n",
    "class UltimateMLP(nn.Module):\n",
    "    def __init__(self, hidden_dim=2048):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            self._make_block(28 * 28, hidden_dim, 0.4),\n",
    "            self._make_block(hidden_dim, hidden_dim//2, 0.35),\n",
    "            self._make_block(hidden_dim//2, hidden_dim//4, 0.3),\n",
    "            nn.Linear(hidden_dim//4, 10)\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_block(self, in_dim, out_dim, dropout):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            Swish(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Swish激活函数的增益值近似为1.1（根据经验值）\n",
    "        gain = 1.1\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8545d5",
   "metadata": {},
   "source": [
    "The final training: I added an optimizer, schedualed the learning rate, and increased the number of training to 250 and choose the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cea5b309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Acc: 96.84%\n",
      "Epoch 2: Val Acc: 97.40%\n",
      "Epoch 3: Val Acc: 97.98%\n",
      "Epoch 4: Val Acc: 97.92%\n",
      "Epoch 5: Val Acc: 97.96%\n",
      "Epoch 6: Val Acc: 98.26%\n",
      "Epoch 7: Val Acc: 98.28%\n",
      "Epoch 8: Val Acc: 97.58%\n",
      "Epoch 9: Val Acc: 98.04%\n",
      "Epoch 10: Val Acc: 98.18%\n",
      "Epoch 11: Val Acc: 98.08%\n",
      "Epoch 12: Val Acc: 97.98%\n",
      "Epoch 13: Val Acc: 98.18%\n",
      "Epoch 14: Val Acc: 98.24%\n",
      "Epoch 15: Val Acc: 98.24%\n",
      "Epoch 16: Val Acc: 98.12%\n",
      "Epoch 17: Val Acc: 98.48%\n",
      "Epoch 18: Val Acc: 98.38%\n",
      "Epoch 19: Val Acc: 98.48%\n",
      "Epoch 20: Val Acc: 98.12%\n",
      "Epoch 21: Val Acc: 98.20%\n",
      "Epoch 22: Val Acc: 98.40%\n",
      "Epoch 23: Val Acc: 98.78%\n",
      "Epoch 24: Val Acc: 98.26%\n",
      "Epoch 25: Val Acc: 98.56%\n",
      "Epoch 26: Val Acc: 98.50%\n",
      "Epoch 27: Val Acc: 98.44%\n",
      "Epoch 28: Val Acc: 98.58%\n",
      "Epoch 29: Val Acc: 98.56%\n",
      "Epoch 30: Val Acc: 98.22%\n",
      "Epoch 31: Val Acc: 98.44%\n",
      "Epoch 32: Val Acc: 98.46%\n",
      "Epoch 33: Val Acc: 98.14%\n",
      "Epoch 34: Val Acc: 98.72%\n",
      "Epoch 35: Val Acc: 98.60%\n",
      "Epoch 36: Val Acc: 98.30%\n",
      "Epoch 37: Val Acc: 98.22%\n",
      "Epoch 38: Val Acc: 98.44%\n",
      "Epoch 39: Val Acc: 98.78%\n",
      "Epoch 40: Val Acc: 98.54%\n",
      "Epoch 41: Val Acc: 98.54%\n",
      "Epoch 42: Val Acc: 98.44%\n",
      "Epoch 43: Val Acc: 98.44%\n",
      "Epoch 44: Val Acc: 98.82%\n",
      "Epoch 45: Val Acc: 98.86%\n",
      "Epoch 46: Val Acc: 98.36%\n",
      "Epoch 47: Val Acc: 98.72%\n",
      "Epoch 48: Val Acc: 98.78%\n",
      "Epoch 49: Val Acc: 98.38%\n",
      "Epoch 50: Val Acc: 98.90%\n",
      "Epoch 51: Val Acc: 98.70%\n",
      "Epoch 52: Val Acc: 98.50%\n",
      "Epoch 53: Val Acc: 98.62%\n",
      "Epoch 54: Val Acc: 98.52%\n",
      "Epoch 55: Val Acc: 98.66%\n",
      "Epoch 56: Val Acc: 98.66%\n",
      "Epoch 57: Val Acc: 98.64%\n",
      "Epoch 58: Val Acc: 98.78%\n",
      "Epoch 59: Val Acc: 98.68%\n",
      "Epoch 60: Val Acc: 98.96%\n",
      "Epoch 61: Val Acc: 98.84%\n",
      "Epoch 62: Val Acc: 98.98%\n",
      "Epoch 63: Val Acc: 98.98%\n",
      "Epoch 64: Val Acc: 98.84%\n",
      "Epoch 65: Val Acc: 98.96%\n",
      "Epoch 66: Val Acc: 98.86%\n",
      "Epoch 67: Val Acc: 98.68%\n",
      "Epoch 68: Val Acc: 98.82%\n",
      "Epoch 69: Val Acc: 98.86%\n",
      "Epoch 70: Val Acc: 98.84%\n",
      "Epoch 71: Val Acc: 98.72%\n",
      "Epoch 72: Val Acc: 98.96%\n",
      "Epoch 73: Val Acc: 98.68%\n",
      "Epoch 74: Val Acc: 98.92%\n",
      "Epoch 75: Val Acc: 99.00%\n",
      "Epoch 76: Val Acc: 98.96%\n",
      "Epoch 77: Val Acc: 98.90%\n",
      "Epoch 78: Val Acc: 98.78%\n",
      "Epoch 79: Val Acc: 99.08%\n",
      "Epoch 80: Val Acc: 98.96%\n",
      "Epoch 81: Val Acc: 98.96%\n",
      "Epoch 82: Val Acc: 98.94%\n",
      "Epoch 83: Val Acc: 98.96%\n",
      "Epoch 84: Val Acc: 98.92%\n",
      "Epoch 85: Val Acc: 99.06%\n",
      "Epoch 86: Val Acc: 98.68%\n",
      "Epoch 87: Val Acc: 98.90%\n",
      "Epoch 88: Val Acc: 99.08%\n",
      "Epoch 89: Val Acc: 99.04%\n",
      "Epoch 90: Val Acc: 98.96%\n",
      "Epoch 91: Val Acc: 99.08%\n",
      "Epoch 92: Val Acc: 98.96%\n",
      "Epoch 93: Val Acc: 98.90%\n",
      "Epoch 94: Val Acc: 99.12%\n",
      "Epoch 95: Val Acc: 99.08%\n",
      "Epoch 96: Val Acc: 99.04%\n",
      "Epoch 97: Val Acc: 98.90%\n",
      "Epoch 98: Val Acc: 99.10%\n",
      "Epoch 99: Val Acc: 99.04%\n",
      "Epoch 100: Val Acc: 98.98%\n",
      "Epoch 101: Val Acc: 99.22%\n",
      "Epoch 102: Val Acc: 98.98%\n",
      "Epoch 103: Val Acc: 98.96%\n",
      "Epoch 104: Val Acc: 99.16%\n",
      "Epoch 105: Val Acc: 99.10%\n",
      "Epoch 106: Val Acc: 99.10%\n",
      "Epoch 107: Val Acc: 99.04%\n",
      "Epoch 108: Val Acc: 99.00%\n",
      "Epoch 109: Val Acc: 98.96%\n",
      "Epoch 110: Val Acc: 99.18%\n",
      "Epoch 111: Val Acc: 99.06%\n",
      "Epoch 112: Val Acc: 99.00%\n",
      "Epoch 113: Val Acc: 98.94%\n",
      "Epoch 114: Val Acc: 99.34%\n",
      "Epoch 115: Val Acc: 99.26%\n",
      "Epoch 116: Val Acc: 99.18%\n",
      "Epoch 117: Val Acc: 99.10%\n",
      "Epoch 118: Val Acc: 99.22%\n",
      "Epoch 119: Val Acc: 99.14%\n",
      "Epoch 120: Val Acc: 99.04%\n",
      "Epoch 121: Val Acc: 99.02%\n",
      "Epoch 122: Val Acc: 99.02%\n",
      "Epoch 123: Val Acc: 99.24%\n",
      "Epoch 124: Val Acc: 99.16%\n",
      "Epoch 125: Val Acc: 99.16%\n",
      "Epoch 126: Val Acc: 99.08%\n",
      "Epoch 127: Val Acc: 99.16%\n",
      "Epoch 128: Val Acc: 99.28%\n",
      "Epoch 129: Val Acc: 99.38%\n",
      "Epoch 130: Val Acc: 99.34%\n",
      "Epoch 131: Val Acc: 99.26%\n",
      "Epoch 132: Val Acc: 99.24%\n",
      "Epoch 133: Val Acc: 99.26%\n",
      "Epoch 134: Val Acc: 99.20%\n",
      "Epoch 135: Val Acc: 99.32%\n",
      "Epoch 136: Val Acc: 99.30%\n",
      "Epoch 137: Val Acc: 99.34%\n",
      "Epoch 138: Val Acc: 99.16%\n",
      "Epoch 139: Val Acc: 99.38%\n",
      "Epoch 140: Val Acc: 99.30%\n",
      "Epoch 141: Val Acc: 99.22%\n",
      "Epoch 142: Val Acc: 99.30%\n",
      "Epoch 143: Val Acc: 99.42%\n",
      "Epoch 144: Val Acc: 99.28%\n",
      "Epoch 145: Val Acc: 99.30%\n",
      "Epoch 146: Val Acc: 99.30%\n",
      "Epoch 147: Val Acc: 99.32%\n",
      "Epoch 148: Val Acc: 99.36%\n",
      "Epoch 149: Val Acc: 99.32%\n",
      "Epoch 150: Val Acc: 99.30%\n",
      "Epoch 151: Val Acc: 99.30%\n",
      "Epoch 152: Val Acc: 99.28%\n",
      "Epoch 153: Val Acc: 99.32%\n",
      "Epoch 154: Val Acc: 99.24%\n",
      "Epoch 155: Val Acc: 99.30%\n",
      "Epoch 156: Val Acc: 99.28%\n",
      "Epoch 157: Val Acc: 99.34%\n",
      "Epoch 158: Val Acc: 99.30%\n",
      "Epoch 159: Val Acc: 99.30%\n",
      "Epoch 160: Val Acc: 99.28%\n",
      "Epoch 161: Val Acc: 99.38%\n",
      "Epoch 162: Val Acc: 99.30%\n",
      "Epoch 163: Val Acc: 99.36%\n",
      "Epoch 164: Val Acc: 99.28%\n",
      "Epoch 165: Val Acc: 99.30%\n",
      "Epoch 166: Val Acc: 99.26%\n",
      "Epoch 167: Val Acc: 99.30%\n",
      "Epoch 168: Val Acc: 99.34%\n",
      "Epoch 169: Val Acc: 99.36%\n",
      "Epoch 170: Val Acc: 99.32%\n",
      "Epoch 171: Val Acc: 99.36%\n",
      "Epoch 172: Val Acc: 99.34%\n",
      "Epoch 173: Val Acc: 99.38%\n",
      "Epoch 174: Val Acc: 99.34%\n",
      "Epoch 175: Val Acc: 99.28%\n",
      "Epoch 176: Val Acc: 99.36%\n",
      "Epoch 177: Val Acc: 99.32%\n",
      "Epoch 178: Val Acc: 99.32%\n",
      "Epoch 179: Val Acc: 99.38%\n",
      "Epoch 180: Val Acc: 99.34%\n",
      "Epoch 181: Val Acc: 99.40%\n",
      "Epoch 182: Val Acc: 99.40%\n",
      "Epoch 183: Val Acc: 99.36%\n",
      "Epoch 184: Val Acc: 99.34%\n",
      "Epoch 185: Val Acc: 99.38%\n",
      "Epoch 186: Val Acc: 99.40%\n",
      "Epoch 187: Val Acc: 99.36%\n",
      "Epoch 188: Val Acc: 99.34%\n",
      "Epoch 189: Val Acc: 99.40%\n",
      "Epoch 190: Val Acc: 99.34%\n",
      "Epoch 191: Val Acc: 99.40%\n",
      "Epoch 192: Val Acc: 99.34%\n",
      "Epoch 193: Val Acc: 99.36%\n",
      "Epoch 194: Val Acc: 99.34%\n",
      "Epoch 195: Val Acc: 99.36%\n",
      "Epoch 196: Val Acc: 99.34%\n",
      "Epoch 197: Val Acc: 99.38%\n",
      "Epoch 198: Val Acc: 99.40%\n",
      "Epoch 199: Val Acc: 99.38%\n",
      "Epoch 200: Val Acc: 99.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6420/3150170951.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最终测试准确率: 99.24%\n"
     ]
    }
   ],
   "source": [
    "# ==================== 训练流程 ====================\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_loader, val_loader, test_loader = get_dataloaders()\n",
    "    \n",
    "    # 模型配置\n",
    "    model = UltimateMLP(hidden_dim=2048).to(device)\n",
    "    \n",
    "    # 优化器配置\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=0.005,\n",
    "        weight_decay=5e-4,\n",
    "        betas=(0.95, 0.99)\n",
    "    )\n",
    "    \n",
    "    # 学习率调度\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=0.02,\n",
    "        pct_start=0.25,\n",
    "        div_factor=25,\n",
    "        final_div_factor=1e4,\n",
    "        total_steps=len(train_loader)*250\n",
    "    )\n",
    "    \n",
    "    criterion = FocalLoss()\n",
    "\n",
    "    # 训练循环\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(250):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # varify \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = total = 0\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_acc = 100 * correct / total\n",
    "            print(f\"Epoch {epoch+1}: Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    # 测试最佳模型\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f\"\\n最终测试准确率: {100 * correct / total:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
